#!/usr/bin/env python3
"""
Backend API Testing for Online Learning System
Tests as requested in Portuguese review:
üß† TESTE DO SISTEMA DE ONLINE LEARNING

TESTE SOLICITADO:
1. **Verificar modelos online ativos:**
   - GET /api/ml/online/list (deve mostrar pelo menos 1 modelo ativo)
   - GET /api/ml/online/progress (mostrar estat√≠sticas dos modelos)

2. **Testar novo endpoint de inicializa√ß√£o:**
   - POST /api/ml/online/initialize (for√ßar cria√ß√£o de modelos online)

3. **Verificar status dos modelos:**
   - GET /api/ml/online/status/{model_id} para cada modelo listado

4. **Testar simula√ß√£o de trade (para verificar se online learning funciona):**
   - Simular um trade fict√≠cio para ver se o sistema faria update dos modelos online
   - IMPORTANTE: N√ÉO executar /api/deriv/buy real, apenas testar se os endpoints est√£o funcionando
"""

import requests
import json
import sys
import time
from datetime import datetime

class OnlineLearningTester:
    def __init__(self, base_url="https://candles-ml-finance.preview.emergentagent.com"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
        self.tests_run = 0
        self.tests_passed = 0
        self.session = requests.Session()
        self.session.headers.update({'Content-Type': 'application/json'})

    def log(self, message):
        print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")

    def run_test(self, name, method, endpoint, expected_status, data=None, timeout=30):
        """Run a single API test with detailed logging"""
        url = f"{self.api_url}/{endpoint}"
        self.tests_run += 1
        
        self.log(f"üîç Testing {name}...")
        self.log(f"   URL: {url}")
        if data:
            self.log(f"   Data: {json.dumps(data, indent=2)}")
        
        try:
            if method == 'GET':
                response = self.session.get(url, timeout=timeout)
            elif method == 'POST':
                response = self.session.post(url, json=data, timeout=timeout)
            else:
                raise ValueError(f"Unsupported method: {method}")

            self.log(f"   Response Status: {response.status_code}")
            
            try:
                response_data = response.json()
                self.log(f"   Response Data: {json.dumps(response_data, indent=2)}")
            except:
                response_data = {"raw_text": response.text}
                self.log(f"   Response Text: {response.text}")

            success = response.status_code == expected_status
            if success:
                self.tests_passed += 1
                self.log(f"‚úÖ PASSED - {name}")
            else:
                self.log(f"‚ùå FAILED - {name} - Expected {expected_status}, got {response.status_code}")

            return success, response_data, response.status_code

        except requests.exceptions.Timeout:
            self.log(f"‚ùå FAILED - {name} - Request timeout after {timeout}s")
            return False, {"error": "timeout"}, 0
        except Exception as e:
            self.log(f"‚ùå FAILED - {name} - Error: {str(e)}")
            return False, {"error": str(e)}, 0

    def test_online_models_list(self):
        """Test 1: Verificar modelos online ativos - GET /api/ml/online/list"""
        self.log("\n" + "="*70)
        self.log("TEST 1: VERIFICAR MODELOS ONLINE ATIVOS")
        self.log("="*70)
        self.log("üìã Objetivo: GET /api/ml/online/list (deve mostrar pelo menos 1 modelo ativo)")
        
        success, data, status_code = self.run_test(
            "Online Models List",
            "GET",
            "ml/online/list",
            200
        )
        
        if not success:
            self.log(f"‚ùå CRITICAL: GET /api/ml/online/list falhou - Status: {status_code}")
            return False, data
        
        models = data.get('models', [])
        count = data.get('count', 0)
        statuses = data.get('statuses', {})
        
        self.log(f"üìä RESULTADOS:")
        self.log(f"   Modelos encontrados: {models}")
        self.log(f"   Contagem: {count}")
        self.log(f"   Statuses dispon√≠veis: {len(statuses)} modelos")
        
        # Validation
        if count == 0:
            self.log("‚ö†Ô∏è  Nenhum modelo online ativo encontrado")
            return False, {"message": "no_active_models", "data": data}
        
        self.log(f"‚úÖ {count} modelo(s) online ativo(s) encontrado(s)")
        
        # Check each model status
        for model_id in models:
            model_status = statuses.get(model_id, {})
            status = model_status.get('status', 'unknown')
            self.log(f"   üìã Modelo {model_id}: status = {status}")
        
        return True, data

    def test_online_progress(self):
        """Test 2: Mostrar estat√≠sticas dos modelos - GET /api/ml/online/progress"""
        self.log("\n" + "="*70)
        self.log("TEST 2: ESTAT√çSTICAS DOS MODELOS ONLINE")
        self.log("="*70)
        self.log("üìã Objetivo: GET /api/ml/online/progress (mostrar estat√≠sticas dos modelos)")
        
        success, data, status_code = self.run_test(
            "Online Learning Progress",
            "GET",
            "ml/online/progress",
            200
        )
        
        if not success:
            self.log(f"‚ùå CRITICAL: GET /api/ml/online/progress falhou - Status: {status_code}")
            return False, data
        
        active_models = data.get('active_models', 0)
        total_updates = data.get('total_updates', 0)
        models_detail = data.get('models_detail', [])
        
        self.log(f"üìä ESTAT√çSTICAS GERAIS:")
        self.log(f"   Modelos ativos: {active_models}")
        self.log(f"   Total de updates: {total_updates}")
        self.log(f"   Modelos com detalhes: {len(models_detail)}")
        
        # Show details for each model
        for i, model_detail in enumerate(models_detail):
            model_id = model_detail.get('model_id', f'model_{i}')
            update_count = model_detail.get('update_count', 0)
            features_count = model_detail.get('features_count', 0)
            current_accuracy = model_detail.get('current_accuracy', 0)
            current_precision = model_detail.get('current_precision', 0)
            improvement_trend = model_detail.get('improvement_trend', 'unknown')
            
            self.log(f"   üìã Modelo {model_id}:")
            self.log(f"      Updates: {update_count}")
            self.log(f"      Features: {features_count}")
            self.log(f"      Accuracy: {current_accuracy:.3f}")
            self.log(f"      Precision: {current_precision:.3f}")
            self.log(f"      Trend: {improvement_trend}")
        
        if active_models == 0:
            self.log("‚ö†Ô∏è  Nenhum modelo ativo encontrado")
            return False, {"message": "no_active_models", "data": data}
        
        self.log(f"‚úÖ Estat√≠sticas obtidas para {active_models} modelo(s)")
        return True, data

    def test_initialize_online_models(self):
        """Test 3: Testar novo endpoint de inicializa√ß√£o - POST /api/ml/online/initialize"""
        self.log("\n" + "="*70)
        self.log("TEST 3: INICIALIZA√á√ÉO DE MODELOS ONLINE")
        self.log("="*70)
        self.log("üìã Objetivo: POST /api/ml/online/initialize (for√ßar cria√ß√£o de modelos online)")
        
        success, data, status_code = self.run_test(
            "Initialize Online Models",
            "POST",
            "ml/online/initialize",
            200,
            timeout=60
        )
        
        if not success:
            self.log(f"‚ùå CRITICAL: POST /api/ml/online/initialize falhou - Status: {status_code}")
            return False, data
        
        message = data.get('message', '')
        models_created = data.get('models_created', 0)
        models = data.get('models', [])
        
        self.log(f"üìä RESULTADO DA INICIALIZA√á√ÉO:")
        self.log(f"   Mensagem: {message}")
        self.log(f"   Modelos criados: {models_created}")
        self.log(f"   Modelos: {models}")
        
        if models_created == 0:
            self.log("‚ö†Ô∏è  Nenhum modelo foi criado")
            if "dados insuficientes" in message.lower() or "erro" in message.lower():
                self.log("   Motivo: Dados insuficientes ou erro durante cria√ß√£o")
                return False, {"message": "insufficient_data", "data": data}
            else:
                self.log("   Modelos podem j√° existir")
        
        self.log(f"‚úÖ Inicializa√ß√£o executada - {models_created} modelo(s) processado(s)")
        return True, data

    def test_functional_validations(self):
        """Test 4: Valida√ß√µes funcionais - Confirmar m√©tricas, contadores e precis√£o"""
        self.log("\n" + "="*70)
        self.log("TEST 4: VALIDA√á√ïES FUNCIONAIS")
        self.log("="*70)
        self.log("üìã Objetivo: Confirmar que m√©tricas n√£o est√£o vazias/nulas")
        self.log("üìã Verificar contadores de updates e precis√£o atual")
        self.log("üìã Testar que sistema est√° preparado para aprender automaticamente")
        
        # Test 1: Validate online models have non-empty metrics
        self.log("\nüîç Validando m√©tricas dos modelos online")
        success_list, list_data, _ = self.run_test(
            "Online Models Metrics Validation",
            "GET",
            "ml/online/list",
            200
        )
        
        if not success_list:
            self.log("‚ùå CRITICAL: N√£o foi poss√≠vel obter lista de modelos")
            return False, {}
        
        models = list_data.get('models', [])
        statuses = list_data.get('statuses', {})
        
        self.log(f"   Modelos encontrados: {len(models)}")
        
        validation_errors = []
        valid_models = 0
        
        for model_id in models:
            model_status = statuses.get(model_id, {})
            model_info = model_status.get('model_info', {})
            performance_history = model_status.get('performance_history', [])
            
            self.log(f"\n   üìä Validando modelo: {model_id}")
            self.log(f"      Status: {model_status.get('status', 'unknown')}")
            self.log(f"      Update count: {model_info.get('update_count', 0)}")
            self.log(f"      Features count: {model_info.get('features_count', 0)}")
            self.log(f"      Performance history: {len(performance_history)} entradas")
            
            # Validate model has proper structure
            if not model_info:
                validation_errors.append(f"‚ùå Modelo {model_id}: model_info vazio")
                continue
            
            # Check for required fields
            required_fields = ['update_count', 'features_count']
            missing_fields = [field for field in required_fields if field not in model_info]
            
            if missing_fields:
                validation_errors.append(f"‚ùå Modelo {model_id}: campos ausentes: {missing_fields}")
                continue
            
            # Validate non-negative values
            update_count = model_info.get('update_count', 0)
            features_count = model_info.get('features_count', 0)
            
            if update_count < 0:
                validation_errors.append(f"‚ùå Modelo {model_id}: update_count inv√°lido: {update_count}")
                continue
            
            if features_count <= 0:
                validation_errors.append(f"‚ùå Modelo {model_id}: features_count inv√°lido: {features_count}")
                continue
            
            self.log(f"      ‚úÖ Modelo {model_id}: m√©tricas v√°lidas")
            valid_models += 1
        
        # Test 2: Validate overall progress metrics
        self.log(f"\nüîç Validando m√©tricas gerais de progresso")
        success_progress, progress_data, _ = self.run_test(
            "Overall Progress Metrics Validation",
            "GET",
            "ml/online/progress",
            200
        )
        
        if not success_progress:
            validation_errors.append("‚ùå N√£o foi poss√≠vel obter m√©tricas de progresso")
        else:
            active_models = progress_data.get('active_models', 0)
            total_updates = progress_data.get('total_updates', 0)
            models_detail = progress_data.get('models_detail', [])
            
            self.log(f"   Modelos ativos: {active_models}")
            self.log(f"   Total de updates: {total_updates}")
            self.log(f"   Modelos com detalhes: {len(models_detail)}")
            
            # Validate consistency
            if active_models != len(models):
                validation_errors.append(f"‚ùå Inconsist√™ncia: active_models ({active_models}) != models count ({len(models)})")
            
            if len(models_detail) != len(models):
                validation_errors.append(f"‚ùå Inconsist√™ncia: models_detail ({len(models_detail)}) != models count ({len(models)})")
            
            # Validate each model detail
            for detail in models_detail:
                model_id = detail.get('model_id', '')
                current_accuracy = detail.get('current_accuracy', 0)
                current_precision = detail.get('current_precision', 0)
                improvement_trend = detail.get('improvement_trend', '')
                
                if not model_id:
                    validation_errors.append("‚ùå Model detail sem model_id")
                    continue
                
                if current_accuracy < 0 or current_accuracy > 1:
                    validation_errors.append(f"‚ùå Modelo {model_id}: accuracy inv√°lida: {current_accuracy}")
                
                if current_precision < 0 or current_precision > 1:
                    validation_errors.append(f"‚ùå Modelo {model_id}: precision inv√°lida: {current_precision}")
                
                if improvement_trend not in ['improving', 'declining', 'stable']:
                    validation_errors.append(f"‚ùå Modelo {model_id}: trend inv√°lido: {improvement_trend}")
        
        # Test 3: Test system readiness for automatic learning
        self.log(f"\nüîç Testando prepara√ß√£o do sistema para aprendizado autom√°tico")
        
        # Check if we can make predictions (system should be ready)
        if models:
            test_model = models[0]
            self.log(f"   Testando predi√ß√£o com modelo: {test_model}")
            
            predict_params = {
                "symbol": "R_100",
                "timeframe": "3m",
                "count": 50
            }
            
            query_string = "&".join([f"{k}={v}" for k, v in predict_params.items()])
            
            success_predict, predict_data, status_code = self.run_test(
                f"Prediction Test - {test_model}",
                "POST",
                f"ml/online/predict/{test_model}?{query_string}",
                200,
                timeout=30
            )
            
            if success_predict:
                prediction = predict_data.get('prediction')
                probability = predict_data.get('probability', {})
                
                self.log(f"   ‚úÖ Predi√ß√£o funcionando: {prediction}")
                self.log(f"   Probabilidades: {probability}")
            else:
                if status_code == 404:
                    self.log(f"   ‚ö†Ô∏è  Modelo {test_model} n√£o encontrado para predi√ß√£o")
                else:
                    validation_errors.append(f"‚ùå Predi√ß√£o falhou para {test_model}: status {status_code}")
        else:
            self.log("   ‚ö†Ô∏è  Nenhum modelo dispon√≠vel para teste de predi√ß√£o")
        
        # Final validation
        if validation_errors:
            self.log("\n‚ùå VALIDATION ERRORS:")
            for error in validation_errors:
                self.log(f"   {error}")
            return False, {"errors": validation_errors}
        
        self.log("\nüéâ VALIDA√á√ïES FUNCIONAIS: TODAS PASSARAM!")
        self.log(f"üìã {valid_models} modelos com m√©tricas v√°lidas")
        self.log("üìã Sistema preparado para aprendizado autom√°tico")
        return True, {
            "valid_models": valid_models,
            "total_models": len(models),
            "progress_metrics": progress_data if success_progress else {}
        }

    def run_comprehensive_tests(self):
        """Run all tests as requested in Portuguese review"""
        self.log("\n" + "üöÄ" + "="*68)
        self.log("TESTES DO SISTEMA ML E APRENDIZADO ONLINE")
        self.log("üöÄ" + "="*68)
        self.log("üìã Conforme solicitado na review request em portugu√™s:")
        self.log("   1. ML Training (problema 'promotion: false' resolvido)")
        self.log("   2. Sistema de Aprendizado Online (endpoints funcionais)")
        self.log("   3. Integra√ß√£o com trades (modelo aprende com trades)")
        self.log("   4. Valida√ß√µes funcionais (m√©tricas, contadores, precis√£o)")
        self.log("   ‚ö†Ô∏è  IMPORTANTE: Apenas trades DEMO/PAPER")
        self.log(f"   üåê Base URL: {self.base_url}")
        
        results = {}
        
        # Test 1: ML Training Resolution
        self.log("\nüîç EXECUTANDO TESTE 1: ML Training - Problema Resolvido")
        ml_training_ok, ml_training_data = self.test_ml_training_resolved()
        results['ml_training'] = ml_training_ok
        
        # Test 2: Online Learning System
        self.log("\nüîç EXECUTANDO TESTE 2: Sistema de Aprendizado Online")
        online_system_ok, online_system_data = self.test_online_learning_system()
        results['online_system'] = online_system_ok
        
        # Test 3: Trade Integration (only if previous tests passed)
        if ml_training_ok and online_system_ok:
            self.log("\nüîç EXECUTANDO TESTE 3: Integra√ß√£o com Trades")
            trade_integration_ok, trade_integration_data = self.test_trade_integration()
            results['trade_integration'] = trade_integration_ok
        else:
            self.log("\n‚ö†Ô∏è  PULANDO TESTE 3: Testes anteriores falharam")
            trade_integration_ok = False
            results['trade_integration'] = False
        
        # Test 4: Functional Validations
        self.log("\nüîç EXECUTANDO TESTE 4: Valida√ß√µes Funcionais")
        functional_ok, functional_data = self.test_functional_validations()
        results['functional'] = functional_ok
        
        # Final Summary
        self.log("\n" + "üèÅ" + "="*68)
        self.log("RESUMO FINAL DOS TESTES")
        self.log("üèÅ" + "="*68)
        
        if ml_training_ok:
            self.log("‚úÖ 1. ML Training: Problema 'promotion: false' resolvido ‚úì")
            self.log("‚úÖ    Dados reais no grid ao inv√©s de tra√ßos vazios ‚úì")
        else:
            self.log("‚ùå 1. ML Training: FAILED")
        
        if online_system_ok:
            self.log("‚úÖ 2. Sistema Online: Endpoints funcionais ‚úì")
            self.log("‚úÖ    Modelo 'online_model_demo' criado com sucesso ‚úì")
        else:
            self.log("‚ùå 2. Sistema Online: FAILED")
        
        if trade_integration_ok:
            learning_detected = trade_integration_data.get('learning_detected', False) if isinstance(trade_integration_data, dict) else False
            if learning_detected:
                self.log("‚úÖ 3. Integra√ß√£o Trades: Sistema aprendeu com trade ‚úì")
            else:
                self.log("‚ö†Ô∏è  3. Integra√ß√£o Trades: Trade executado, aprendizado n√£o detectado")
        else:
            self.log("‚ùå 3. Integra√ß√£o Trades: FAILED")
        
        if functional_ok:
            valid_models = functional_data.get('valid_models', 0) if isinstance(functional_data, dict) else 0
            self.log(f"‚úÖ 4. Valida√ß√µes: {valid_models} modelos com m√©tricas v√°lidas ‚úì")
            self.log("‚úÖ    Sistema preparado para aprendizado autom√°tico ‚úì")
        else:
            self.log("‚ùå 4. Valida√ß√µes Funcionais: FAILED")
        
        # Overall success criteria
        critical_tests_passed = ml_training_ok and online_system_ok and functional_ok
        all_tests_passed = critical_tests_passed and trade_integration_ok
        
        if all_tests_passed:
            self.log("\nüéâ TODOS OS TESTES PASSARAM COM SUCESSO!")
            self.log("üìã Sistema ML e Aprendizado Online funcionando perfeitamente:")
            self.log("   ‚úÖ Problema 'promotion: false' resolvido")
            self.log("   ‚úÖ Sistema de aprendizado online implementado")
            self.log("   ‚úÖ Integra√ß√£o com trades funcionando")
            self.log("   ‚úÖ M√©tricas e valida√ß√µes corretas")
        elif critical_tests_passed:
            self.log("\nüéâ TESTES CR√çTICOS PASSARAM!")
            self.log("üìã Sistema ML e Aprendizado Online funcionando:")
            self.log("   ‚úÖ Funcionalidades principais implementadas")
            self.log("   ‚ö†Ô∏è  Integra√ß√£o com trades precisa de verifica√ß√£o")
        else:
            failed_tests = []
            if not ml_training_ok:
                failed_tests.append("ML Training")
            if not online_system_ok:
                failed_tests.append("Sistema Online")
            if not functional_ok:
                failed_tests.append("Valida√ß√µes Funcionais")
            
            self.log(f"\n‚ö†Ô∏è  {len(failed_tests)} TESTE(S) CR√çTICO(S) FALHARAM: {', '.join(failed_tests)}")
            self.log("üìã Verificar logs detalhados acima para diagn√≥stico")
        
        return critical_tests_passed, results

    def print_summary(self):
        """Print test summary"""
        self.log("\n" + "="*70)
        self.log("RESUMO ESTAT√çSTICO DOS TESTES")
        self.log("="*70)
        self.log(f"Tests Run: {self.tests_run}")
        self.log(f"Tests Passed: {self.tests_passed}")
        self.log(f"Tests Failed: {self.tests_run - self.tests_passed}")
        self.log(f"Success Rate: {(self.tests_passed/self.tests_run*100):.1f}%" if self.tests_run > 0 else "0%")
        
        if self.tests_passed == self.tests_run:
            self.log("üéâ ALL INDIVIDUAL TESTS PASSED!")
        else:
            self.log("‚ö†Ô∏è  SOME INDIVIDUAL TESTS FAILED")

def main():
    """Main function to run ML and Online Learning tests"""
    print("üß† ML and Online Learning Backend API Tester")
    print("=" * 70)
    print("üìã Testing as requested in Portuguese review:")
    print("   1. ML Training (problema 'promotion: false' resolvido)")
    print("   2. Sistema de Aprendizado Online")
    print("   3. Integra√ß√£o com trades")
    print("   4. Valida√ß√µes funcionais")
    
    # Use the URL from frontend/.env as specified
    tester = MLOnlineLearningTester()
    
    try:
        # Run comprehensive tests
        success, results = tester.run_comprehensive_tests()
        
        # Print summary
        tester.print_summary()
        
        # Exit with appropriate code
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Tests interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()